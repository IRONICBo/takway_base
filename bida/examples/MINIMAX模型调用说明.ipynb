{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MINIMAX 支持说明\n",
    "MiniMax分为两种调用模式：\n",
    "1. ChatCompletion ，简称cc模式，调用方式基本与其他模型一样\n",
    "2. ChatCompletion Pro，简称ccp模式，支持多人多bot对话场景，示例对话，返回格式限制，函数调用，插件等功能，调用模式和其他模型有很大不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatCompletion ，简称cc模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正常调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "\n",
    "llm = ChatLLM(model_type='minimax')\n",
    "\n",
    "result = llm.chat(\"你好，你是一个服务助理，请简洁回答我的问题。我怎么称呼你？\", \n",
    "                  role_meta={\"user_name\": \"老周\",\"bot_name\": \"小A\"}) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.chat(\"你怎么称呼我？\") \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 流式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    print(data, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax\", model_name='5.0', stream_callback=my_stream_process_data) \n",
    "\n",
    "llm.chat(\"从1连续加到100，和是多少？\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.chat(\"如果只计算1到100之间奇数的和呢？\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.chat(\"算的不对\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    print(data, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax\", stream_callback=my_stream_process_data) \n",
    "\n",
    "result = llm.chat(\"从1连续加到100，和是多少？\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.chat(\"如果只计算1到100之间奇数的和呢？\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.chat(\"算的不对\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatCompletion Pro 调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prompt必须使用minimax.MessageStrWrapper进行包装，添加用户名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调用模型：https://api.minimax.chat/v1/text/chatcompletion_pro?GroupId=1768536805574983864，\n",
      "请求参数：{'model': 'abab5.5-chat', 'tokens_to_generate': 1024, 'stream': True, 'use_standard_sse': True, 'temperature': 0.01, 'messages': [{'sender_name': '用户', 'text': '从1加到100，答案多少？说一下思路', 'sender_type': 'USER'}], 'role_meta': {'user_name': '我', 'bot_name': '专家'}, 'bot_setting': [{'bot_name': 'MM智能助理', 'content': 'MM智能助理是一款由MiniMax自研的，没有调用其他产品的接口的大型语言模型。MiniMax是一家中国科技公司，一直致力于进行大模型相关的研究。'}], 'reply_constraints': {'sender_type': 'BOT', 'sender_name': 'MM智能助理'}}\n",
      "从1加到100，我们可以使用等差数列求和公式来计算。公式为：n*(n+1)/2，其中n是项数。在这个问题中，n=100，所以答案是100*(100+1)/2=5050。"
     ]
    }
   ],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    print(data, end=\"\", flush=True)\n",
    "\n",
    "# llm = ChatLLM(model_type=\"minimax-pro\") \n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\", stream_callback=my_stream_process_data) \n",
    "\n",
    "message = minimax.MessageStrWrapper(text=\"从1加到100，答案多少？说一下思路\", \n",
    "                                    sender_name=\"用户\")\n",
    "\n",
    "result = llm.chat(prompt=message) \n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调用模型：https://api.minimax.chat/v1/text/chatcompletion_pro?GroupId=1768536805574983864，\n",
      "请求参数：{'model': 'abab5.5-chat', 'tokens_to_generate': 1024, 'stream': True, 'use_standard_sse': True, 'temperature': 0.01, 'messages': [{'sender_type': 'USER', 'sender_name': '用户', 'text': '从1加到100，答案多少？说一下思路'}, {'sender_type': 'BOT', 'sender_name': 'MM智能助理', 'text': '从1加到100，我们可以使用等差数列求和公式来计算。公式为：n*(n+1)/2，其中n是项数。在这个问题中，n=100，所以答案是100*(100+1)/2=5050。'}, {'sender_name': '用户', 'text': '如果只计算1到100之间奇数的和呢？', 'sender_type': 'USER'}], 'role_meta': {'user_name': '我', 'bot_name': '专家'}, 'bot_setting': [{'bot_name': 'MM智能助理', 'content': 'MM智能助理是一款由MiniMax自研的，没有调用其他产品的接口的大型语言模型。MiniMax是一家中国科技公司，一直致力于进行大模型相关的研究。'}], 'reply_constraints': {'sender_type': 'BOT', 'sender_name': 'MM智能助理'}}\n",
      "如果只计算1到100之间奇数的和，我们可以使用等差数列求和公式来计算。在这个问题中，n=50（因为1到100之间有50个奇数），所以答案是50*(50+1)/2=1275。"
     ]
    }
   ],
   "source": [
    "message = minimax.MessageStrWrapper(text=\"如果只计算1到100之间奇数的和呢？\", \n",
    "                                    sender_name=\"用户\")\n",
    "result = llm.chat(message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调用模型：https://api.minimax.chat/v1/text/chatcompletion_pro?GroupId=1768536805574983864，\n",
      "请求参数：{'model': 'abab5.5-chat', 'tokens_to_generate': 1024, 'stream': True, 'use_standard_sse': True, 'temperature': 0.01, 'messages': [{'sender_type': 'USER', 'sender_name': '用户', 'text': '从1加到100，答案多少？说一下思路'}, {'sender_type': 'BOT', 'sender_name': 'MM智能助理', 'text': '从1加到100，我们可以使用等差数列求和公式来计算。公式为：n*(n+1)/2，其中n是项数。在这个问题中，n=100，所以答案是100*(100+1)/2=5050。'}, {'sender_type': 'USER', 'sender_name': '用户', 'text': '如果只计算1到100之间奇数的和呢？'}, {'sender_type': 'BOT', 'sender_name': 'MM智能助理', 'text': '如果只计算1到100之间奇数的和，我们可以使用等差数列求和公式来计算。在这个问题中，n=50（因为1到100之间有50个奇数），所以答案是50*(50+1)/2=1275。'}, {'sender_name': '用户', 'text': '算的不对', 'sender_type': 'USER'}], 'role_meta': {'user_name': '我', 'bot_name': '专家'}, 'bot_setting': [{'bot_name': 'MM智能助理', 'content': 'MM智能助理是一款由MiniMax自研的，没有调用其他产品的接口的大型语言模型。MiniMax是一家中国科技公司，一直致力于进行大模型相关的研究。'}], 'reply_constraints': {'sender_type': 'BOT', 'sender_name': 'MM智能助理'}}\n",
      "对不起，我犯了一个错误。正确的答案是50*(50+1)/2=1275。"
     ]
    }
   ],
   "source": [
    "message = minimax.MessageStrWrapper(text=\"算的不对\", \n",
    "                                    sender_name=\"用户\")\n",
    "result = llm.chat(message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bot设定和多角色回复（bot setting和reply constraints）\n",
    "\n",
    "其实每次调用都需要传递bot setting和reply constraints，前面示例没有是因为做了默认设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    print(data, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\", stream_callback=my_stream_process_data) \n",
    "\n",
    "sender_name=\"老周\"\n",
    "message = minimax.MessageStrWrapper(text=\"你好，请问你是谁？\", \n",
    "                                    sender_name=sender_name)\n",
    "\n",
    "bot_setting = [ \n",
    "        {\n",
    "            \"bot_name\": \"MM智能助理\",\n",
    "            \"content\": \"MM智能助理是一款由MiniMax自研的，没有调用其他产品的接口的大型语言模型。MiniMax是一家中国科技公司，一直致力于进行大模型相关的研究。\"\n",
    "        },\n",
    "        {\n",
    "            \"bot_name\": \"M&M\",\n",
    "            \"content\": \"M&M是巧克力豆，特别好吃~\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "reply_constraints={ \n",
    "        \"sender_type\": \"BOT\",\n",
    "        \"sender_name\": \"MM智能助理\"\n",
    "    }\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  bot_setting=bot_setting,\n",
    "                  reply_constraints=reply_constraints,\n",
    "                  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 示例对话功能（sample messages）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    print(data, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\", stream_callback=my_stream_process_data) \n",
    "\n",
    "sender_name=\"老周\"\n",
    "message = minimax.MessageStrWrapper(text=\"1+1=\", \n",
    "                                    sender_name=sender_name)\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_messages = [\n",
    "        {\n",
    "            \"sender_type\": \"USER\",\n",
    "            \"sender_name\": \"小明\",\n",
    "            \"text\": \"10*10=\"\n",
    "        },\n",
    "        {\n",
    "            \"sender_type\": \"BOT\",\n",
    "            \"sender_name\": \"MM智能助理\",\n",
    "            \"text\": \"10*10=100\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    print(data, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\", stream_callback=my_stream_process_data) \n",
    "\n",
    "sender_name=\"老周\"\n",
    "message = minimax.MessageStrWrapper(text=\"1+1=\", \n",
    "                                    sender_name=sender_name)\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  sample_messages=sample_messages,\n",
    "                  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 限制返回格式功能（glyph）\n",
    "\n",
    "注意：glyph 与 stream / plugins / functions 暂不支持同时开启"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "import json\n",
    "import copy\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\") \n",
    "\n",
    "sender_name=\"老周\"\n",
    "message = minimax.MessageStrWrapper(text=\"请用英语帮我翻译下面这句话：我是谁\", \n",
    "                                    sender_name=sender_name)\n",
    "\n",
    "# 使用SDK中的默认值\n",
    "bot_setting = copy.copy(minimax.Default_Bot_Setting)\n",
    "reply_constraints = copy.copy(minimax.Default_Reply_Constraints)\n",
    "reply_constraints[\"glyph\"] = {\n",
    "            \"type\": \"raw\",\n",
    "            \"raw_glyph\": \"这句话的翻译是：{{gen 'content'}}\"\n",
    "        }\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  bot_setting=bot_setting,\n",
    "                  reply_constraints=reply_constraints,\n",
    "                  ) \n",
    "print(json.loads(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "import json\n",
    "import copy\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\") \n",
    "\n",
    "sender_name=\"用户\"\n",
    "message = minimax.MessageStrWrapper(text=\"小明今年25岁，他的女朋友小华比他小一岁\", \n",
    "                                    sender_name=sender_name)\n",
    "\n",
    "# 使用SDK中的默认值\n",
    "bot_setting = copy.copy(minimax.Default_Bot_Setting)\n",
    "reply_constraints = copy.copy(minimax.Default_Reply_Constraints)\n",
    "reply_constraints[\"glyph\"] = {\n",
    "            \"type\": \"json_value\",\n",
    "            \"json_properties\": {\n",
    "                \"name\": {\n",
    "                  \"type\": \"string\"\n",
    "                  },\n",
    "                \"age\": {\n",
    "                  \"type\": \"number\"\n",
    "                  }\n",
    "            }\n",
    "        }\n",
    "# 每次调用只返回一个结果，这个示例调用两次\n",
    "result = llm.chat(prompt=message,\n",
    "                  bot_setting=bot_setting,\n",
    "                  reply_constraints=reply_constraints,\n",
    "                  ) \n",
    "print(json.loads(result))\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  bot_setting=bot_setting,\n",
    "                  reply_constraints=reply_constraints,\n",
    "                  ) \n",
    "print(json.loads(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数调用功能（functions）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "from bida import ManagementFunctions\n",
    "\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    if isinstance(data, str):           \n",
    "        print(data+'', end=\"\", flush=True)\n",
    "    else:\n",
    "        print(f'函数调用中：{data}\\n......')\n",
    "\n",
    "# llm = ChatLLM(model_type=\"minimax-pro\") \n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\", stream_callback=my_stream_process_data) \n",
    "\n",
    "message = minimax.MessageStrWrapper(text=\"北京的天气？\", \n",
    "                                    sender_name=\"用户\")\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  functions=ManagementFunctions.get_functions(),  # 添加自定义函数\n",
    "                  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 插件功能（plugins）【bida暂不支持，开发中】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bida import ChatLLM\n",
    "from bida.models.minimax_sdk import minimax\n",
    "from bida import ManagementFunctions\n",
    "\n",
    "\n",
    "def my_stream_process_data(data):            \n",
    "    if isinstance(data, str):           \n",
    "        print(data+'', end=\"\", flush=True)\n",
    "    else:\n",
    "        print(f'函数调用中：{data}\\n......')\n",
    "\n",
    "llm = ChatLLM(model_type=\"minimax-pro\") \n",
    "\n",
    "# llm = ChatLLM(model_type=\"minimax-pro\", stream_callback=my_stream_process_data) \n",
    "\n",
    "message = minimax.MessageStrWrapper(text=\"北京的天气？\", \n",
    "                                    sender_name=\"用户\")\n",
    "\n",
    "result = llm.chat(prompt=message,\n",
    "                  max_tokens=2048,\n",
    "                  plugins = [\n",
    "                              \"plugin_web_search\"\n",
    "                            ]\n",
    "                  ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
